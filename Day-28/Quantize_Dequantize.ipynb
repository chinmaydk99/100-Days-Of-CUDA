{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ORiqux8yTCnO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def quantize_kernel(\n",
        "    x_ptr,        # Pointer to the input tensor (float32)\n",
        "    output_ptr,   # Pointer to the output tensor (int8)\n",
        "    scale_ptr,    # Pointer to the scaling factor\n",
        "    n_elements,   # Number of elements in the tensor\n",
        "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n",
        "):\n",
        "    # Program ID\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Calculate the start offset for this program\n",
        "    offset = pid * BLOCK_SIZE\n",
        "\n",
        "    # Create a mask to handle the case where n_elements is not a multiple of BLOCK_SIZE\n",
        "    mask = offset + tl.arange(0, BLOCK_SIZE) < n_elements\n",
        "\n",
        "    # Load input x\n",
        "    x = tl.load(x_ptr + offset, mask=mask)\n",
        "\n",
        "    # Load scale\n",
        "    scale = tl.load(scale_ptr)\n",
        "\n",
        "    # Quantize: scale and clamp\n",
        "    quantized = tl.math.round(x / scale)\n",
        "    quantized = tl.math.min(127, tl.math.max(-128, quantized))\n",
        "\n",
        "    # Store output\n",
        "    tl.store(output_ptr + offset, quantized, mask=mask)\n",
        "\n",
        "@triton.jit\n",
        "def dequantize_kernel(\n",
        "    quant_ptr,     # Pointer to the quantized tensor (int8)\n",
        "    output_ptr,    # Pointer to the output tensor (float32)\n",
        "    scale_ptr,     # Pointer to the scaling factor\n",
        "    n_elements,    # Number of elements in the tensor\n",
        "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n",
        "):\n",
        "    # Program ID\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Calculate the start offset for this program\n",
        "    offset = pid * BLOCK_SIZE\n",
        "\n",
        "    # Create a mask to handle the case where n_elements is not a multiple of BLOCK_SIZE\n",
        "    mask = offset + tl.arange(0, BLOCK_SIZE) < n_elements\n",
        "\n",
        "    # Load quantized input\n",
        "    quant = tl.load(quant_ptr + offset, mask=mask)\n",
        "\n",
        "    # Load scale\n",
        "    scale = tl.load(scale_ptr)\n",
        "\n",
        "    # Dequantize: multiply by scale\n",
        "    dequantized = quant * scale\n",
        "\n",
        "    # Store output\n",
        "    tl.store(output_ptr + offset, dequantized, mask=mask)"
      ],
      "metadata": {
        "id": "_lInHNnGUsq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper functions to call the triton kernels\n",
        "def quantize(x, scale=None):\n",
        "    \"\"\"\n",
        "    Quantize a tensor from float32 to int8 using a per-tensor scaling factor.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor (float32)\n",
        "        scale: Optional scaling factor. If None, will be calculated as max(abs(x))/127\n",
        "\n",
        "    Returns:\n",
        "        Quantized tensor (int8) and the scale used\n",
        "    \"\"\"\n",
        "    if scale is None:\n",
        "        scale = torch.max(torch.abs(x)) / 127.0\n",
        "\n",
        "    # Ensure scale is a tensor\n",
        "    if not isinstance(scale, torch.Tensor):\n",
        "        scale = torch.tensor([scale], device=x.device, dtype=torch.float32)\n",
        "\n",
        "    # Output tensor\n",
        "    output = torch.empty_like(x, dtype=torch.int8)\n",
        "\n",
        "    # Calculate grid and block sizes\n",
        "    n_elements = x.numel()\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
        "\n",
        "    # Launch kernel\n",
        "    quantize_kernel[grid, 1](\n",
        "        x.data_ptr(),\n",
        "        output.data_ptr(),\n",
        "        scale.data_ptr(),\n",
        "        n_elements,\n",
        "        BLOCK_SIZE,\n",
        "    )\n",
        "\n",
        "    return output, scale"
      ],
      "metadata": {
        "id": "6LJBqlWvUtMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dequantize(x_quant, scale):\n",
        "    # Ensure scale is a tensor\n",
        "    if not isinstance(scale, torch.Tensor):\n",
        "        scale = torch.tensor([scale], device=x_quant.device, dtype=torch.float32)\n",
        "\n",
        "    # Output tensor\n",
        "    output = torch.empty_like(x_quant, dtype=torch.float32)\n",
        "\n",
        "    # Calculate grid and block sizes\n",
        "    n_elements = x_quant.numel()\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
        "\n",
        "    # Launch kernel\n",
        "    dequantize_kernel[grid, 1](\n",
        "        x_quant.data_ptr(),\n",
        "        output.data_ptr(),\n",
        "        scale.data_ptr(),\n",
        "        n_elements,\n",
        "        BLOCK_SIZE,\n",
        "    )\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "3SbZsP6dUzwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random tensor\n",
        "x = torch.randn(1000000, device=\"cuda\")\n",
        "\n",
        "# Quantize\n",
        "x_quant, scale = quantize(x)\n",
        "\n",
        "# Dequantize\n",
        "x_dequant = dequantize(x_quant, scale)\n",
        "\n",
        "# Calculate error\n",
        "error = torch.abs(x - x_dequant).mean()\n",
        "print(f\"Mean absolute error: {error.item()}\")"
      ],
      "metadata": {
        "id": "UW0MfCKBUQbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}