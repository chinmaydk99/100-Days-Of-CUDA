{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gNqMJaREvDeA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.autotune(\n",
        "        configs=[\n",
        "            triton.Config({}, num_stages=1, num_warps=8),\n",
        "            triton.Config({}, num_stages=2, num_warps=8),\n",
        "            triton.Config({}, num_stages=4, num_warps=8),\n",
        "            triton.Config({}, num_stages=8, num_warps=8),\n",
        "            triton.Config({}, num_stages=1),\n",
        "            triton.Config({}, num_stages=2),\n",
        "            triton.Config({}, num_stages=4),\n",
        "            triton.Config({}, num_stages=8),\n",
        "            triton.Config({}, num_warps=1),\n",
        "            triton.Config({}, num_warps=2),\n",
        "            triton.Config({}, num_warps=4),\n",
        "            triton.Config({}, num_warps=8),\n",
        "        ],\n",
        "        key=[\"n_elements\"],\n",
        "    )\n",
        "@triton.jit\n",
        "def quant_columnwise_kernel(input_ptr,\n",
        "                            output_ptr,\n",
        "                            col_max_ptr,\n",
        "                            n_elements,\n",
        "                            M : tl.constexpr,\n",
        "                            N : tl.constexpr,\n",
        "                            BLOCK_SIZE : tl.constexpr,\n",
        "                            NP2 : tl.constexpr):\n",
        "    pid = tl.program_id(0)\n",
        "    block_start = pid # For row wise this would be pid*BLOCK_SIZE\n",
        "    arange = tl.arange(0, NP2)\n",
        "    offsets = block_start  + arange * N # row_idx * num_columns + col_idx\n",
        "    mask = arange < M\n",
        "\n",
        "    input = tl.load(input_ptr + offsets, mask=mask)\n",
        "    inp_abs = tl.abs(input)\n",
        "    col_max = tl.max(tl.where(mask, inp_abs, 0), axis = 0)\n",
        "\n",
        "    output = tl.extra.cuda.libdevice.round(127.0 * (input / col_max))\n",
        "\n",
        "    new_start = pid*M\n",
        "    new_offset = arange + new_start # Would be storing it at r_Idx*n_cols + c_idx. Its transposed so storing it at c_idx * n_rows + r_iidx\n",
        "\n",
        "    tl.store(output_ptr + new_offset, output, mask=mask)\n",
        "    tl.store(col_max_ptr + pid , col_max)"
      ],
      "metadata": {
        "id": "7Z8FR8uRAKUT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quant_columnwise_transpose(x:torch.Tensor):\n",
        "    M, N = x.shape\n",
        "    output = torch.empty(N, M, dtype=torch.int8, device=x.device)\n",
        "    col_maxs = torch.empty(x.shape[1], dtype=torch.float16, device=x.device)\n",
        "\n",
        "    NP2 = triton.next_power_of_2(M)\n",
        "\n",
        "    assert x.is_cuda and output.is_cuda\n",
        "    n_elements = x.numel()\n",
        "\n",
        "    # grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
        "    grid = lambda meta: (N,)\n",
        "    quant_columnwise_kernel[grid](x,\n",
        "                                  output,\n",
        "                                  col_maxs,\n",
        "                                  n_elements,\n",
        "                                  M = M,\n",
        "                                  N =  N,\n",
        "                                  BLOCK_SIZE = M,\n",
        "                                  NP2 = NP2)\n",
        "\n",
        "    return output, col_maxs"
      ],
      "metadata": {
        "id": "OV1zth1W-1BX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M, N = 256, 128\n",
        "input_tensor = torch.randn((M, N), device=\"cuda\")\n",
        "\n",
        "\n",
        "quantized_output, col_maxs = quant_columnwise_transpose(input_tensor)\n",
        "col_maxs = col_maxs.unsqueeze(0)\n",
        "quantized_output = torch.transpose(quantized_output, 0, 1)"
      ],
      "metadata": {
        "id": "40wT4WG_CtdK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_output.shape, col_maxs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiG_yG65jFZs",
        "outputId": "5466f6e3-9c0f-40ca-cbce-287ebf1f7d00"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([256, 128]), torch.Size([1, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected_maxs = input_tensor.abs().max(dim=0, keepdim=True)[0]\n",
        "expected_output = (127.0 * (input_tensor / expected_maxs)).round().to(torch.int8)\n",
        "\n",
        "expected_maxs = expected_maxs.to(torch.float16)\n",
        "\n",
        "expected_output.shape, expected_maxs.shape"
      ],
      "metadata": {
        "id": "bCC2MuaHD0QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc5cae6-0da3-4968-fafa-4724be6c1266"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([256, 128]), torch.Size([1, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.allclose(quantized_output, expected_output) and torch.allclose(col_maxs, expected_maxs.squeeze(0)):\n",
        "    print(\"✅ Triton column-wise kernel matches PyTorch output!\")\n",
        "else:\n",
        "    print(\"❌ Mismatch detected!\")\n",
        "    print(\"Max absolute difference:\", (quantized_output - expected_output).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBeGXYq7jKUt",
        "outputId": "a17afc79-de9e-4160-b538-8c99e636d609"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Triton column-wise kernel matches PyTorch output!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mhctBfijh9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}